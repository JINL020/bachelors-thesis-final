{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tensorflow 2.5!!!\n",
    "# !conda install tensorflow==2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect from an Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CUSTOM_MODEL_NAME = 'my_ssd_mobilenet'\n",
    "\n",
    "paths = {\n",
    "    'APIMODEL_PATH': os.path.join('Tensorflow','models'),\n",
    "    'SCRIPTS_PATH': os.path.join('Tensorflow','scripts'),\n",
    "\n",
    "    'WORKSPACE_PATH': os.path.join('Tensorflow', 'workspace'),\n",
    "\n",
    "    'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace','annotations'),\n",
    "    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images'),\n",
    "\n",
    "    'MODEL_PATH': os.path.join('Tensorflow', 'workspace','models'),\n",
    "    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME),\n",
    "    'OUTPUT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'),\n",
    "    'TFLITE_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfliteexport')\n",
    " }\n",
    "\n",
    "files = {\n",
    "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], 'label_map.pbtxt'),\n",
    "    'TRAINING_SCRIPT': os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'model_main_tf2.py')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to run custom TFLite model on test images to detect objects\n",
    "# Source: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py\n",
    "\n",
    "# Import packages\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import importlib.util\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "### Define function for inferencing with TFLite model and displaying results\n",
    "\n",
    "def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=1, savepath='/content/results', txt_only=False):\n",
    "\n",
    "  # Grab filenames of all images in test folder\n",
    "  images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n",
    "\n",
    "  # Load the label map into memory\n",
    "  with open(lblpath, 'r') as f:\n",
    "      labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "  # Load the Tensorflow Lite model into memory\n",
    "  interpreter = Interpreter(model_path=modelpath)\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  # Get model details\n",
    "  input_details = interpreter.get_input_details()\n",
    "  output_details = interpreter.get_output_details()\n",
    "  height = input_details[0]['shape'][1]\n",
    "  width = input_details[0]['shape'][2]\n",
    "\n",
    "  float_input = (input_details[0]['dtype'] == np.float32)\n",
    "\n",
    "  input_mean = 127.5\n",
    "  input_std = 127.5\n",
    "\n",
    "  # Randomly select test images\n",
    "  images_to_test = random.sample(images, num_test_images)\n",
    "\n",
    "  # Loop over every image and perform detection\n",
    "  for image_path in images_to_test:\n",
    "\n",
    "      # Load image and resize to expected shape [1xHxWx3]\n",
    "      image = cv2.imread(image_path)\n",
    "      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "      imH, imW, _ = image.shape\n",
    "      image_resized = cv2.resize(image_rgb, (width, height))\n",
    "      input_data = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "      # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
    "      if float_input:\n",
    "          input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "      # Perform the actual detection by running the model with the image as input\n",
    "      interpreter.set_tensor(input_details[0]['index'],input_data)\n",
    "      interpreter.invoke()\n",
    "\n",
    "      # Retrieve detection results\n",
    "      boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects\n",
    "      classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects\n",
    "      scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence of detected objects      \n",
    "\n",
    "      detections = []\n",
    "\n",
    "      # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
    "      for i in range(len(scores)):\n",
    "          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
    "\n",
    "              # Get bounding box coordinates and draw box\n",
    "              # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
    "              ymin = int(max(1,(boxes[i][0] * imH)))\n",
    "              xmin = int(max(1,(boxes[i][1] * imW)))\n",
    "              ymax = int(min(imH,(boxes[i][2] * imH)))\n",
    "              xmax = int(min(imW,(boxes[i][3] * imW)))\n",
    "              \n",
    "              print(ymin)\n",
    "              print(xmin)\n",
    "              print(ymax)\n",
    "              print(xmax)\n",
    "\n",
    "              cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
    "\n",
    "              # Draw label\n",
    "              object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n",
    "              label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
    "              labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
    "              label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
    "              cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
    "              cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
    "\n",
    "              detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n",
    "\n",
    "\n",
    "      # All the results have been drawn on the image, now display the image\n",
    "      if txt_only == False: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        plt.figure(figsize=(12,16))\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "\n",
    "      # Save detection results in .txt files (for calculating mAP)\n",
    "      elif txt_only == True:\n",
    "\n",
    "        # Get filenames and paths\n",
    "        image_fn = os.path.basename(image_path)\n",
    "        base_fn, ext = os.path.splitext(image_fn)\n",
    "        txt_result_fn = base_fn +'.txt'\n",
    "        txt_savepath = os.path.join(savepath, txt_result_fn)\n",
    "\n",
    "        # Write results to text file\n",
    "        # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n",
    "        with open(txt_savepath,'w') as f:\n",
    "            for detection in detections:\n",
    "                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float32' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m images_to_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m   \u001b[38;5;66;03m# Number of images to run detection on\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Run inferencing function!\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtflite_detect_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_TO_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH_TO_IMAGES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH_TO_LABELS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_conf_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_to_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 74\u001b[0m, in \u001b[0;36mtflite_detect_images\u001b[1;34m(modelpath, imgpath, lblpath, min_conf, num_test_images, savepath, txt_only)\u001b[0m\n\u001b[0;32m     71\u001b[0m detections \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Loop over all detections and draw detection box if confidence is above minimum threshold\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ((scores[i] \u001b[38;5;241m>\u001b[39m min_conf) \u001b[38;5;129;01mand\u001b[39;00m (scores[i] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m)):\n\u001b[0;32m     76\u001b[0m \n\u001b[0;32m     77\u001b[0m         \u001b[38;5;66;03m# Get bounding box coordinates and draw box\u001b[39;00m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;66;03m# Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\u001b[39;00m\n\u001b[0;32m     79\u001b[0m         ymin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m,(boxes[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m imH)))\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'numpy.float32' has no len()"
     ]
    }
   ],
   "source": [
    "# Set up variables for running user's model\n",
    "PATH_TO_IMAGES=os.path.join('Tensorflow', 'workspace','images', 'test')   # Path to test images folder\n",
    "PATH_TO_MODEL='licence_model_my.tflite'   # Path to .tflite model file\n",
    "PATH_TO_LABELS=os.path.join('Tensorflow', 'workspace','annotations','label_map.txt')   # Path to labelmap.txt file\n",
    "min_conf_threshold=0.5   # Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
    "images_to_test = 1   # Number of images to run detection on\n",
    "\n",
    "# Run inferencing function!\n",
    "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Saved Model to TFLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELMAP_FILE = os.path.join('Tensorflow', 'workspace','annotations','label_map.txt')\n",
    "# OUTPUT_MODEL_PATH = \"updated_model.tflite\"\n",
    "OUTPUT_MODEL_PATH = \"licence_model_my.tflite\"\n",
    "OUTPUT_MODEL_PATH_JSON = \"licence_model_my.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "SAVED_MODEL_PATH = os.path.join('Tensorflow', 'workspace','models', 'my_ssd_mobilenet', 'tfliteexport', 'saved_model') \n",
    "\n",
    "# Convert the model to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model with metadata\n",
    "# tflite_model_path = os.path.join('Tensorflow', 'workspace','models', 'my_ssd_mobilenet', 'tfliteexport', 'saved_model', 'tflite_model_with_metadata.tflite') \n",
    "tflite_model_path = os.path.join(OUTPUT_MODEL_PATH) \n",
    "\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Metadata to TFLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tflite_support import flatbuffers\n",
    "from tflite_support import metadata as _metadata\n",
    "from tflite_support import metadata_schema_py_generated as _metadata_fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model meta data\n",
    "model_meta = _metadata_fb.ModelMetadataT()\n",
    "model_meta.name = \"my_ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\"\n",
    "model_meta.description = (\"This model detects car licence plates and is based on the ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8 model\")\n",
    "model_meta.version = \"1.0\"\n",
    "model_meta.author = \"Jin-Jin Lee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input tensor info\n",
    "input_meta = _metadata_fb.TensorMetadataT()\n",
    "input_meta.name = \"image\"\n",
    "input_meta.description = (\n",
    "    \"Input image to be classified.\\n\"\n",
    "    \"One input: image, a float32 tensor of shape[1, height, width, 3] containing \"\n",
    "    \"the *normalized* input image. The expected image is 320 x 320, with three channels (red, blue, and green) per pixel\"\n",
    ")\n",
    "\n",
    "# Set content properties for feature\n",
    "input_meta.content = _metadata_fb.ContentT()\n",
    "input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()\n",
    "input_meta.content.contentProperties.colorSpace = (_metadata_fb.ColorSpaceType.RGB)\n",
    "input_meta.content.contentPropertiesType = (_metadata_fb.ContentProperties.ImageProperties)\n",
    "\n",
    "input_normalization = _metadata_fb.ProcessUnitT()\n",
    "input_normalization.optionsType = (_metadata_fb.ProcessUnitOptions.NormalizationOptions)\n",
    "input_normalization.options = _metadata_fb.NormalizationOptionsT()\n",
    "input_normalization.options.mean = [127.5]\n",
    "input_normalization.options.std = [127.5]\n",
    "\n",
    "input_meta.processUnits = [input_normalization]\n",
    "\n",
    "input_stats = _metadata_fb.StatsT()\n",
    "input_stats.max = [255]\n",
    "input_stats.min = [0]\n",
    "\n",
    "input_meta.stats = input_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create first output tensor info (locations)\n",
    "location_meta = _metadata_fb.TensorMetadataT()\n",
    "location_meta.name = \"locations\"\n",
    "location_meta.description = \"The locations of the detected boxes.\"\n",
    "\n",
    "# Set content properties for bounding box\n",
    "location_meta.content = _metadata_fb.ContentT()\n",
    "location_meta.content.contentProperties = _metadata_fb.BoundingBoxPropertiesT()\n",
    "location_meta.content.contentProperties.index = [1, 0, 3, 2]\n",
    "location_meta.content.contentProperties.type = _metadata_fb.BoundingBoxType.BOUNDARIES\n",
    "location_meta.content.contentPropertiesType = _metadata_fb.ContentProperties.BoundingBoxProperties\n",
    "\n",
    "\n",
    "# Set range\n",
    "location_meta.content.range = _metadata_fb.ValueRangeT()\n",
    "location_meta.content.range.min = 2\n",
    "location_meta.content.range.max = 2\n",
    "\n",
    "# Index 0: Corresponds to the x-coordinate of the top-left corner (x_min).\n",
    "# Index 1: Corresponds to the y-coordinate of the top-left corner (y_min).\n",
    "# Index 2: Corresponds to the x-coordinate of the bottom-right corner (x_max).\n",
    "# Index 3: Corresponds to the y-coordinate of the bottom-right corner (y_max).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create second output tensor info (classes)\n",
    "classes_meta = _metadata_fb.TensorMetadataT()\n",
    "classes_meta.name= \"classes\"\n",
    "classes_meta.description = \"The classes of the detected boxes.\"\n",
    "\n",
    "classes_meta.content = _metadata_fb.ContentT()\n",
    "classes_meta.content.contentProperties = _metadata_fb.FeaturePropertiesT()\n",
    "classes_meta.content.contentPropertiesType = _metadata_fb.ContentProperties.FeatureProperties\n",
    "\n",
    "label_file = _metadata_fb.AssociatedFileT()\n",
    "label_file.name = os.path.basename(LABELMAP_FILE)\n",
    "label_file.description = \"Labels for objects that the model can recognize.\"\n",
    "label_file.type = _metadata_fb.AssociatedFileType.TENSOR_VALUE_LABELS\n",
    "\n",
    "classes_meta.associatedFiles = [label_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create third output tensor info (scores)\n",
    "scores_meta = _metadata_fb.TensorMetadataT()\n",
    "scores_meta.name= \"scores\"\n",
    "scores_meta.description = \"The scores of the detected boxes.\"\n",
    "\n",
    "scores_meta.content = _metadata_fb.ContentT()\n",
    "scores_meta.content.contentProperties = _metadata_fb.FeaturePropertiesT()\n",
    "scores_meta.content.contentPropertiesType = _metadata_fb.ContentProperties.FeatureProperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create forth output tensor info (number of detections)\n",
    "num_meta = _metadata_fb.TensorMetadataT()\n",
    "num_meta.name= \"number of detections\"\n",
    "num_meta.description = \"The number of the detected boxes.\"\n",
    "\n",
    "num_meta.content = _metadata_fb.ContentT()\n",
    "num_meta.content.contentProperties = _metadata_fb.FeaturePropertiesT()\n",
    "num_meta.content.contentPropertiesType = _metadata_fb.ContentProperties.FeatureProperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates subgraph info.\n",
    "subgraph = _metadata_fb.SubGraphMetadataT()\n",
    "subgraph.inputTensorMetadata = [input_meta]\n",
    "subgraph.outputTensorMetadata = [location_meta, classes_meta, scores_meta, num_meta]\n",
    "model_meta.subgraphMetadata = [subgraph]\n",
    "\n",
    "# Initializes a FlatBuffers Builder object with an initial size of 0\n",
    "builder = flatbuffers.Builder(0)\n",
    "builder.Finish(model_meta.Pack(builder), _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n",
    "metadata_buf = builder.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "populator = _metadata.MetadataPopulator.with_model_file(OUTPUT_MODEL_PATH)\n",
    "populator.load_metadata_buffer(metadata_buf)\n",
    "populator.load_associated_files([LABELMAP_FILE])\n",
    "populator.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayer = _metadata.MetadataDisplayer.with_model_file(OUTPUT_MODEL_PATH)\n",
    "\n",
    "json_file = displayer.get_metadata_json()\n",
    "# Optional: write out the metadata as a json file\n",
    "with open(OUTPUT_MODEL_PATH_JSON, \"w\") as f:\n",
    "  f.write(json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
